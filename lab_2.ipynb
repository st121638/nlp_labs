{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac20a85-a714-4359-9808-a596c91fe35f",
   "metadata": {},
   "source": [
    "## Вариант 1\n",
    "\n",
    "1. Используйте тексты из предыдущего задания и обучите на них модель векторного представления слов (опционально можно приводить слова к нормальной форме и удалить стоп-слова). Можно использовать `gensim`.\n",
    "\n",
    "2. Разделите коллекцию текстов на обучающее и тестовое множество. С помощью обученной модели векторного представления отобразите каждый документ в вектор, усреднив все вектора для слов документа. \n",
    "\n",
    "3. Используйте какой-либо алгоритм классификации (например `SVM`) для классификации текстов. Для обучения используйте тестовое множество, для анализа результатов - тестовое.\n",
    "\n",
    "4. Простое усреднение векторов слов обычно не дает хорошего отображения документа в вектор. Придумайте альтернативный способ. Протестируйте его, повторно обучив алгоритм классификации на тех же данных. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7551c7-f93e-49d9-bae9-5d573d47a1b1",
   "metadata": {},
   "source": [
    "## 1) Обучим w2v и удалим стоп слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "484aade2-5300-4b0b-ba82-8b15fd7cf330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterator, List\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "349f7fbf-bd66-4f78-83a6-5db0f07b1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#считаем данные\n",
    "WORDVEC_LEN = 300\n",
    "SEED = 0\n",
    "\n",
    "@dataclass\n",
    "class Text:\n",
    "    label: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "def read_texts(fn: str) -> Iterator[Text]:\n",
    "    with gzip.open(fn, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield Text(*line.strip().split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e22cc975-2cb5-4e94-ac19-01f994480a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество 10000\n"
     ]
    }
   ],
   "source": [
    "texts = list(read_texts(\"data/news.txt.gz\"))\n",
    "print(f\"Количество {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb7deb82-d7a7-4f52-8996-aa615af9e179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bayut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# загрузим стоп-слова\n",
    "nltk.download('stopwords')\n",
    "stopwords_ru = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ef6d7f5-c638-44a5-9b36-857d176bd9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разобъём текст по словам\n",
    "def tokenize_text(text: str) -> List[str]:\n",
    "    text = text.lower()\n",
    "    words = nltk.WordPunctTokenizer().tokenize(text)\n",
    "    return words\n",
    "\n",
    "#нормализация текста, удаление знаков препинаний и стоп слов\n",
    "def normalize_text(words: str) -> str:\n",
    "    filtered_words = [w for w in words if all(c not in string.punctuation for c in w)]\n",
    "    words = [word for word in filtered_words if word not in stopwords_ru]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9883bab-2c96-45ec-9033-dcd122c0e905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: \n",
      "Парусная гонка Giraglia Rolex Cup пройдет в Средиземном море в 64-й раз. Победители соревнования, проводимого с 1953 года Yacht Club Italiano, помимо других призов традиционно получают в подарок часы от швейцарского бренда Rolex. Об этом сообщается в пресс-релизе, поступившем в редакцию «Ленты.ру» в среду, 8 мая. Rolex Yacht-Master 40 Фото: пресс-служба Mercury Соревнования будут проходить с 10 по 18 июня. Первый этап: ночной переход из Сан-Ремо в Сен-Тропе 10-11 июня (дистанция 50 морских миль — около 90 километров). Второй этап: серия прибрежных гонок в бухте Сен-Тропе с 11 по 14 июня. Финальный этап пройдет с 15 по 18 июня: оффшорная гонка по маршруту Сен-Тропе — Генуя (243 морских мили — 450 километров). Маршрут проходит через скалистый остров Джиралья к северу от Корсики и завершается в Генуе.Регата, с 1997 года проходящая при поддержке Rolex, считается одной из самых значительных яхтенных гонок в Средиземноморье. В этом году в ней ожидается участие трех российских экипажей.\n",
      "После токенизации и нормализации: \n",
      "парусная гонка giraglia rolex cup пройдет средиземном море 64 й победители соревнования проводимого 1953 года yacht club italiano помимо других призов традиционно получают подарок часы швейцарского бренда rolex сообщается пресс релизе поступившем редакцию « ленты ру » среду 8 мая rolex yacht master 40 фото пресс служба mercury соревнования будут проходить 10 18 июня первый этап ночной переход сан ремо сен тропе 10 11 июня дистанция 50 морских миль — около 90 километров второй этап серия прибрежных гонок бухте сен тропе 11 14 июня финальный этап пройдет 15 18 июня оффшорная гонка маршруту сен тропе — генуя 243 морских мили — 450 километров маршрут проходит скалистый остров джиралья северу корсики завершается генуе регата 1997 года проходящая поддержке rolex считается одной самых значительных яхтенных гонок средиземноморье году ожидается участие трех российских экипажей\n"
     ]
    }
   ],
   "source": [
    "sample_text = texts[0].text\n",
    "tokens = tokenize_text(sample_text)\n",
    "normalized = normalize_text(tokens)\n",
    "\n",
    "print(f\"Исходный текст: \\n{sample_text}\")\n",
    "print(f\"После токенизации и нормализации: \\n{' '.join(normalized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc724a9a-ad55-4567-9f34-d189315feb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выполнено!\n"
     ]
    }
   ],
   "source": [
    "# нормализация данных для word2vec\n",
    "sentences = [normalize_text(tokenize_text(text.text)) for text in texts]\n",
    "\n",
    "# обучение модели\n",
    "w2v = Word2Vec(sentences, vector_size=WORDVEC_LEN, sg=1, seed=SEED, workers=1, min_count=50, window=50)\n",
    "\n",
    "print(\"Выполнено!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e97ef35f-4c53-4101-b13f-46def6935280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры похожих слов:\n",
      "Для слова 'новости': [('риа', 0.9235125780105591), ('интерфакс', 0.5452293157577515), ('тасс', 0.5287728905677795), ('сергей', 0.4358040392398834), ('сегодня', 0.4345269799232483), ('владимир', 0.4302092492580414), ('дмитрий', 0.42994630336761475), ('рф', 0.4222298860549927), ('алексея', 0.417143851518631), ('агентства', 0.3930620551109314)]\n",
      "Для слова 'спорт': [('р', 0.6705067753791809), ('экспресс', 0.6555298566818237), ('советский', 0.62300705909729), ('сборной', 0.5687049031257629), ('виталий', 0.5682256817817688), ('спорта', 0.5678483247756958), ('тренер', 0.5645512342453003), ('чемпионат', 0.562313973903656), ('чемпионате', 0.5578190088272095), ('мутко', 0.5553413033485413)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Примеры похожих слов:\")\n",
    "print(\"Для слова 'новости':\", w2v.wv.most_similar(\"новости\"))\n",
    "print(\"Для слова 'спорт':\", w2v.wv.most_similar(\"спорт\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f21f1d1-f387-437c-b1d8-99233f94c8cf",
   "metadata": {},
   "source": [
    "## 2) Разделите коллекцию текстов на обучающее и тестовое множество. С помощью обученной модели векторного представления отобразите каждый документ в вектор, усреднив все вектора для слов документа. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3bcbfeb-f1e6-4cda-a9fa-3bc3e36ddc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [text.text for text in texts]\n",
    "y = [text.label for text in texts]\n",
    "\n",
    "raw_train, raw_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c114a3c-527f-4bae-ab26-42637c5e381f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: 8000\n",
      "Размер тестовой выборки: 2000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размер обучающей выборки: {len(raw_train)}\")\n",
    "print(f\"Размер тестовой выборки: {len(raw_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b86ccbac-91a0-488d-821f-9978e1ce83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_emb_avg(doc):\n",
    "    res = np.zeros(WORDVEC_LEN)\n",
    "    tokens = normalize_text(tokenize_text(doc))\n",
    "    valid_tokens = [word for word in tokens if word in w2v.wv.key_to_index]\n",
    "    if not valid_tokens:\n",
    "        return res\n",
    "    for word in valid_tokens:\n",
    "        res += w2v.wv.get_vector(word)\n",
    "    return res / len(valid_tokens)\n",
    "\n",
    "X_train = [get_doc_emb_avg(doc) for doc in raw_train]\n",
    "X_test = [get_doc_emb_avg(doc) for doc in raw_test]\n",
    "\n",
    "#нормализация\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3af50cf-ddf9-4a1d-b4c8-038a0349b122",
   "metadata": {},
   "source": [
    "## 3. Обучим Naive Bayes классификатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd5c6ec9-3057-462c-8131-abc7b8a981aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bayut\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выполнено!\n"
     ]
    }
   ],
   "source": [
    "#переведём метки в числовые обозначения\n",
    "topics = {'science':0, 'style':1, 'culture':2, 'life':3, 'economics':4, \n",
    "          'business':5, 'travel':6, 'forces':7, 'media':8, 'sport':9}\n",
    "\n",
    "y_train = list(map(topics.get, y_train))\n",
    "y_test = list(map(topics.get, y_test))\n",
    "\n",
    "# настройка и обучение классификатора\n",
    "param_grid = [{'alpha': 0.0001 * np.arange(1,11)}]\n",
    "multi_roc = make_scorer(roc_auc_score, average='weighted', multi_class='ovr', needs_proba=True)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "search = GridSearchCV(clf, param_grid, cv=5, scoring=multi_roc)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Выполнено!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eaf6ee5e-62af-410f-9870-8c085fc0fc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры: {'alpha': 0.0001}\n",
      "ROC-AUC: 0.9667054965725029\n"
     ]
    }
   ],
   "source": [
    "print(f\"Лучшие параметры: {search.best_params_}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, search.best_estimator_.predict_proba(X_test), multi_class='ovr')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf097ac-92af-4166-8e65-4829892be4db",
   "metadata": {},
   "source": [
    "## 4) Протестируем улучшенный метод с использованием TF-IDF и w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cbec925-b42c-4831-8252-a2c49c4014f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bayut\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выполнено!\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: normalize_text(tokenize_text(x)))\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "#eменьшаем размерность\n",
    "svd = TruncatedSVD(n_components=WORDVEC_LEN, random_state=SEED)\n",
    "X_dense = svd.fit_transform(X_tfidf.T)\n",
    "\n",
    "# cоздаем словарь эмбеддингов\n",
    "keys = vectorizer.get_feature_names_out()\n",
    "emb_dict = {keys[i]: X_dense[i] for i in range(len(keys))}\n",
    "\n",
    "def get_doc_emb_with_tfidf(doc):\n",
    "    res = np.zeros(WORDVEC_LEN)\n",
    "    tokens = normalize_text(tokenize_text(doc))\n",
    "    if not tokens:\n",
    "        return res\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in emb_dict:\n",
    "            res += emb_dict[word]\n",
    "            count += 1\n",
    "        if word in w2v.wv.key_to_index:\n",
    "            res += w2v.wv.get_vector(word)\n",
    "            count += 1\n",
    "    return res / max(count, 1)\n",
    "\n",
    "# новые эмбеддинги\n",
    "X_train_improved = [get_doc_emb_with_tfidf(doc) for doc in raw_train]\n",
    "X_test_improved = [get_doc_emb_with_tfidf(doc) for doc in raw_test]\n",
    "\n",
    "# нормализация\n",
    "X_train_improved = scaler.fit_transform(X_train_improved)\n",
    "X_test_improved = scaler.transform(X_test_improved)\n",
    "\n",
    "# обучаем классификатор\n",
    "search.fit(X_train_improved, y_train)\n",
    "improved_score = roc_auc_score(y_test, search.best_estimator_.predict_proba(X_test_improved), multi_class='ovr')\n",
    "\n",
    "print(\"Выполнено!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73490eb7-c93d-4cbc-bb50-8269a755ea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (улучшенный метод): 0.9643417560145897\n"
     ]
    }
   ],
   "source": [
    "print(f\"ROC-AUC (улучшенный метод): {improved_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c60de29-f604-4ccc-b51a-f1e6136445b3",
   "metadata": {},
   "source": [
    "## Можно сделать вывод, что алгоритм w2v уже эффективно учитывает контекст слов. Улучшенный метод в данном случае не привёл к значимому улучшению метрики качества"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
